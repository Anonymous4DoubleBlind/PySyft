{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Inference on ResNet-18\n",
    "\n",
    "_Encrypted Machine Learning as a Service_ allows owners of sensitive data to use external AI services to get insights over their data. Let's consider a practical scenario where a data owner holds private images and would like to use a service to have those images labeled, without disclosing the images or the labels, and without having to get access the model, which is often considered to be a business asset by such services and is therefore not accessible.\n",
    "\n",
    "To get a realistic example, we will consider [the task of distinguishing between bees and ants](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), which uses a ResNet-18 model to achieve around 95% accuracy. We won't consider training such model, as we assume the AI service provider has already done this training using some data. Instead, we will showcase how we can use PySyft to encrypt both the model and some image samples and to label those images in a fully private way.\n",
    "\n",
    "Author:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Did you just say _encrypted_?\n",
    "\n",
    "First, let's try to understand what mechanisms we use to make the data and the model private. If you want to jump straight to the code, you can skip this section! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show me the code\n",
    "\n",
    "Enough explications, let's open the code!\n",
    "We will first load the data and the model and store them on the `data_owner` and the `model_owner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We download the data and load it on a DataLoader with small batches of size 2, to reduce the inference time and the memory pressure on the RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "#!unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_dataset = datasets.ImageFolder('hymenoptera_data/val', data_transform)\n",
    "dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_size = len(image_dataset)\n",
    "class_names = image_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wan't to have a look at your data? Check the samples on [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Now let's download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-1_M81rMYoB1A8_nKXr0BBOwSIKXPp2v' -O lol.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You can also download the file_ [here](https://drive.google.com/file/d/1-1_M81rMYoB1A8_nKXr0BBOwSIKXPp2v/view?usp=sharing) _if the command above is not working._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "# Here the size of each output sample is set to 2.\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "state = torch.load(\"./resnet18_ants_bees.pt\", map_location='cpu')\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "# This is a small trick because these two consecuting operations can be switch without\n",
    "# changing the result but it reduces the number of comparisons we have to compute\n",
    "model.maxpool, model.relu = model.relu, model.maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we're ready to start!\n",
    "\n",
    "### Virtual Setup\n",
    "\n",
    "First let's create a virtual setup with 2 workers names `data_owner` and `model_owner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch) \n",
    "data_owner = sy.VirtualWorker(hook, id=\"data_owner\")\n",
    "model_owner = sy.VirtualWorker(hook, id=\"model_owner\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove compression to have faster communication\n",
    "from syft.serde.compression import NO_COMPRESSION\n",
    "sy.serde.compression.default_compress_scheme = NO_COMPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put some data on the `data_owner` and the model on the `model_owner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, true_labels = next(iter(dataloader))\n",
    "data_ptr = data.send(data_owner)\n",
    "\n",
    "# We store the true output of the model for comparison purpose\n",
    "true_prediction = model(data)\n",
    "model_ptr = model.send(model_owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, when calling `.send()`, we only have access to pointers to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor | me:5019812250 -> data_owner:11452442965]\n"
     ]
    }
   ],
   "source": [
    "print(data_ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encryption time!\n",
    "\n",
    "We will now encrypt both the model and the data. To do this, we encrypt them remotely using the pointers and get back the encrypted objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encryption_kwargs = dict(\n",
    "    workers=(data_owner, model_owner),\n",
    "    crypto_provider=crypto_provider,\n",
    "    protocol=\"fss\",\n",
    "    precision_fractional=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_data = data_ptr.encrypt(**encryption_kwargs).get()\n",
    "encrypted_model = model_ptr.encrypt(**encryption_kwargs).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure inference\n",
    "We are now able to run our secure inference, so let's do it and let's compare it to the `true_labels` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tryffel/code/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py:122: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.64794588088989 seconds\n",
      "Predicted labels: tensor([1.])\n",
      "     True labels: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "encrypted_prediction = encrypted_model(encrypted_data)\n",
    "encrypted_labels = encrypted_prediction.argmax(dim=1)\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "\n",
    "labels = encrypted_labels.decrypt()\n",
    "\n",
    "print(\"Predicted labels:\", labels)\n",
    "print(\"     True labels:\", true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! This works!! Well at least with a probability of 95%...\n",
    "\n",
    "But is the computation _exactly_ the same than the plaintext model? Well not exactly, because we sometime use approximations, but let's open the model output logits to verify how close we are from plaintext execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9186,  1.6685]])\n",
      "tensor([[-3.0606,  1.8032]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(encrypted_prediction.decrypt())\n",
    "print(true_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, this is quite close and in practice the accuracy of the model is preserved.\n",
    "\n",
    "Regarding **runtime**, we manage to predict a batch of 2 images in ~400 seconds, which isn't super fast but is already reasonable for our usecase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "> Ok that's good, but in real life I won't use virtual workers!\n",
    "\n",
    "That's right, actually you can run exactly the same experiment using PyGrid and workers which live in a PrivateGridNetwork.\n",
    "\n",
    "To do so, first clone [PyGrid](https://github.com/OpenMined/PyGrid) and then start new nodes in your terminal (one by tab) as such:\n",
    "```\n",
    "cd PyGrid/apps/node\n",
    "./run.sh --id data_owner      --port 7600 --host localhost --start_local_db\n",
    "./run.sh --id model_owner     --port 7601 --host localhost --start_local_db\n",
    "./run.sh --id crypto_provider --port 7602 --host localhost --start_local_db\n",
    "```\n",
    "\n",
    "And you replace the `syft` imports as such:\n",
    "```\n",
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "data_owner = DataCentricFLClient(hook, \"ws://localhost:7600\")\n",
    "model_owner = DataCentricFLClient(hook, \"ws://localhost:7601\")\n",
    "crypto_provider = DataCentricFLClient(hook, \"ws://localhost:7602\")\n",
    "\n",
    "my_grid = sy.PrivateGridNetwork(data_owner, model_owner, crypto_provider)\n",
    "```\n",
    "\n",
    "The computation will be exactly the same, and the runtime will roughtly double. You can run the experiment to verify this, and it's a nice intro to PyGrid! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's  next?\n",
    "\n",
    "Next is improving this first proof of concept! How can this be done?\n",
    "\n",
    "- First, we can optimize our implementation, for example by switching for Python to Rust.\n",
    "- Second, we can try to adapt the model structure or model layers to have a faster execution given our constraints without compromising accuracy. Think of the swap we made between maxpool and relu in the ResNet-18 architecture at thhe beginning.\n",
    "- Last, we can investigate new Function Secret Sharing crypto protocols, this is a new and promising field, we expect new breakthroughs to help us improving the inference time!\n",
    "\n",
    "### Join us!\n",
    "\n",
    "If you want to help, come and [apply to join one of our cryptography teams](https://forms.gle/BWmYQJrCwqe1m3ex5)!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "You can also help our community by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pysyft)",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
